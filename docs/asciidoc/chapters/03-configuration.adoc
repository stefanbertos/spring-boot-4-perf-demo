= Configuration

== Configuration Overview

The application uses Spring Boot's externalized configuration, supporting multiple configuration sources:

. `application.yaml` (default configuration)
. `application-{profile}.yaml` (profile-specific)
. Environment variables
. Command-line arguments

== Spring Cloud Config Server

The application uses a **3-layer configuration approach** to support local development, Docker Compose, and Kubernetes deployments consistently.

=== Layer 1: Local application.yml

Each module contains a local `application.yml` in `src/main/resources/`. This serves as the baseline configuration for local development without requiring the config-server to be running.

[source,text]
----
perf-tester/src/main/resources/application.yml
ibm-mq-consumer/src/main/resources/application.yml
kafka-consumer/src/main/resources/application.yml
----

=== Layer 2: Config-repo Files

Centralized configuration files in `infrastructure/config-repo/` are served by the config-server module (port 8888). These are used when running with Docker Compose.

[source,text]
----
infrastructure/config-repo/
├── application.yml                    # Shared config for all services
├── application-kubernetes.yml         # Shared Kubernetes overrides
├── perf-tester.yml                    # perf-tester specific config
├── perf-tester-kubernetes.yml         # perf-tester Kubernetes overrides
├── ibm-mq-consumer.yml               # ibm-mq-consumer specific config
├── ibm-mq-consumer-kubernetes.yml     # ibm-mq-consumer Kubernetes overrides
├── kafka-consumer.yml                 # kafka-consumer specific config
└── kafka-consumer-kubernetes.yml      # kafka-consumer Kubernetes overrides
----

The shared `application.yml` contains defaults inherited by all services (virtual threads, actuator endpoints, logging levels). Service-specific files override or extend these defaults.

=== Layer 3: Helm ConfigMap

For Kubernetes deployments, all config-repo files are embedded in the config-server Helm ConfigMap at `infrastructure/helm/config-server/templates/configmap.yaml`. This eliminates the need for a Git repository or volume mount in the cluster.

The ConfigMap must include both base files (`<service>.yml`) and Kubernetes profile files (`<service>-kubernetes.yml`), because client applications run with `SPRING_PROFILES_ACTIVE=kubernetes`.

=== Configuration Consistency

When changing any configuration property, all three layers must be updated:

[cols="1,3,2"]
|===
|Layer |Path |Used When

|Local
|`<module>/src/main/resources/application.yml`
|Local dev without config-server

|Config-repo
|`infrastructure/config-repo/<service>.yml`
|Docker Compose with config-server

|Helm ConfigMap
|`infrastructure/helm/config-server/templates/configmap.yaml`
|Kubernetes / ArgoCD deployment
|===

== Avro Serialization

The project uses **Apache Avro** for binary serialization of Kafka messages, providing compact encoding and schema evolution support without requiring a Schema Registry.

=== Avro Schema

The shared `avro-common` module contains the `MqMessage` schema and custom serializers. The schema defines two fields:

.avro-common/src/main/avro/MqMessage.avsc
[source,json]
----
{
  "namespace": "com.example.avro",
  "type": "record",
  "name": "MqMessage",
  "fields": [
    { "name": "content", "type": "string" },
    { "name": "timestamp", "type": { "type": "long", "logicalType": "timestamp-millis" } }
  ]
}
----

The `timestamp-millis` logical type maps to `java.time.Instant` in the generated Java code.

=== Custom Serializers

Since the project does not use a Schema Registry, custom serializer and deserializer classes handle Avro encoding directly:

[cols="1,3"]
|===
|Class |Fully Qualified Name

|Serializer
|`com.example.avro.serialization.AvroSerializer`

|Deserializer
|`com.example.avro.serialization.AvroDeserializer`
|===

These are configured as Kafka producer/consumer value serializers in the Spring configuration:

[source,yaml]
----
spring:
  kafka:
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: com.example.avro.serialization.AvroSerializer
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: com.example.avro.serialization.AvroDeserializer
----

=== Module Dependency

Both `ibm-mq-consumer` and `kafka-consumer` depend on the `avro-common` module to access the generated `MqMessage` class and the serialization classes.

== Redis Configuration

The `perf-tester` module uses Redis for caching performance test results and metrics.

=== Connection Settings

Redis connection is configured via Spring Data Redis properties:

[source,yaml]
----
spring:
  data:
    redis:
      host: localhost
      port: 6379
----

=== Environment-Specific Hosts

[cols="1,2,2"]
|===
|Environment |Host |Port

|Local development
|`localhost`
|6379

|Docker Compose
|`perf-redis`
|6379

|Kubernetes
|Configured via environment variables
|6379
|===

In Kubernetes, the Redis host is injected through the Helm values and environment variables in the deployment template, allowing it to point to the cluster-internal Redis service.

== Module Configuration

=== perf-tester Configuration

.application.yaml
[source,yaml]
----
server:
  port: 8080

spring:
  application:
    name: perf-tester
  threads:
    virtual:
      enabled: true

# IBM MQ Configuration
ibm:
  mq:
    queue-manager: QM1
    channel: DEV.APP.SVRCONN
    conn-name: localhost(1414)
    user: app
    password: passw0rd

# Queues
app:
  mq:
    inbound-queue: DEV.QUEUE.1
    outbound-queue: DEV.QUEUE.2

# Observability
management:
  endpoints:
    web:
      exposure:
        include: health,info,prometheus,metrics
  tracing:
    sampling:
      probability: 1.0
----

=== ibm-mq-consumer Configuration

.application.yaml
[source,yaml]
----
server:
  port: 8081

spring:
  application:
    name: ibm-mq-consumer
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: com.example.avro.serialization.AvroSerializer
    consumer:
      group-id: ibm-mq-consumer-group
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: com.example.avro.serialization.AvroDeserializer

# IBM MQ Configuration
ibm:
  mq:
    queue-manager: QM1
    channel: DEV.APP.SVRCONN
    conn-name: localhost(1414)
    user: app
    password: passw0rd

# Kafka Topics
app:
  kafka:
    request-topic: mq-requests
    response-topic: mq-responses
  mq:
    inbound-queue: DEV.QUEUE.2
    outbound-queue: DEV.QUEUE.1
----

=== kafka-consumer Configuration

.application.yaml
[source,yaml]
----
server:
  port: 8082

spring:
  application:
    name: kafka-consumer
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: com.example.avro.serialization.AvroSerializer
    consumer:
      group-id: kafka-consumer-group
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: com.example.avro.serialization.AvroDeserializer

app:
  kafka:
    request-topic: mq-requests
    response-topic: mq-responses
----

=== api-gateway Configuration

.application.yaml
[source,yaml]
----
server:
  port: 8090

spring:
  application:
    name: api-gateway
  cloud:
    gateway:
      server:
        webmvc:
          routes:
            - id: grafana
              uri: ${GRAFANA_URL:http://localhost:3000}
              predicates:
                - Path=/grafana/**
              filters:
                - StripPrefix=0

            - id: prometheus
              uri: ${PROMETHEUS_URL:http://localhost:9090}
              predicates:
                - Path=/prometheus/**
              filters:
                - StripPrefix=0

            - id: loki
              uri: ${LOKI_URL:http://localhost:3100}
              predicates:
                - Path=/loki/**
              filters:
                - StripPrefix=1

            - id: ibmmq
              uri: ${IBMMQ_URL:https://localhost:9443}
              predicates:
                - Path=/ibmmq/**
              filters:
                - StripPrefix=1

            - id: kafdrop
              uri: ${KAFDROP_URL:http://localhost:9000}
              predicates:
                - Path=/kafdrop/**
              filters:
                - StripPrefix=0

            - id: perf-tester
              uri: ${PERF_TESTER_URL:http://localhost:8080}
              predicates:
                - Path=/api/**
              filters:
                - StripPrefix=0

            - id: sonar
              uri: ${SONAR_URL:http://localhost:9000}
              predicates:
                - Path=/sonar/**
              filters:
                - StripPrefix=1
----

== Environment Variables

=== Application Environment Variables

[cols="3,2,4"]
|===
|Variable |Default |Description

|`SPRING_PROFILES_ACTIVE`
|default
|Active Spring profile (docker, kubernetes)

|`SERVER_PORT`
|varies
|HTTP server port

|`JAVA_TOOL_OPTIONS`
|`-XX:+UseZGC -XX:+UseContainerSupport -XX:MaxRAMPercentage=75.0 -XX:InitialRAMPercentage=75.0`
|JVM options (read automatically by the JVM)
|===

=== IBM MQ Environment Variables

[cols="3,2,4"]
|===
|Variable |Default |Description

|`IBM_MQ_QUEUE_MANAGER`
|QM1
|Queue manager name

|`IBM_MQ_CHANNEL`
|DEV.APP.SVRCONN
|MQ channel name

|`IBM_MQ_CONN_NAME`
|localhost(1414)
|Connection string

|`IBM_MQ_USER`
|app
|MQ username

|`IBM_MQ_PASSWORD`
|passw0rd
|MQ password
|===

=== Kafka Environment Variables

[cols="3,2,4"]
|===
|Variable |Default |Description

|`SPRING_KAFKA_BOOTSTRAP_SERVERS`
|localhost:9092
|Kafka broker addresses

|`SPRING_KAFKA_CONSUMER_GROUP_ID`
|varies
|Consumer group ID
|===

=== Observability Environment Variables

[cols="3,2,4"]
|===
|Variable |Default |Description

|`OTEL_EXPORTER_OTLP_ENDPOINT`
|http://localhost:4317
|OpenTelemetry collector endpoint

|`MANAGEMENT_TRACING_SAMPLING_PROBABILITY`
|1.0
|Trace sampling rate (0.0-1.0)
|===

== Spring Profiles

=== default Profile

Used for local development with services running on localhost.

=== docker Profile

Used when running in Docker Compose. Updates service URLs to use Docker network hostnames.

.Activating docker profile
[source,bash]
----
# Via environment variable
export SPRING_PROFILES_ACTIVE=docker

# Via command line
./gradlew bootRun --args='--spring.profiles.active=docker'
----

=== kubernetes Profile

Used for Kubernetes deployment. Services are accessed via Kubernetes service names.

== Logging Configuration

=== Log Levels

[source,yaml]
----
logging:
  level:
    root: INFO
    com.example: DEBUG
    org.springframework.jms: INFO
    org.springframework.kafka: INFO
    com.ibm.mq: WARN
----

=== Debug Logging

Enable detailed logging for troubleshooting:

[source,yaml]
----
logging:
  level:
    org.springframework.jms: DEBUG
    com.ibm.mq: DEBUG
    com.ibm.msg: DEBUG
    org.springframework.kafka: DEBUG
----

=== Log Format (JSON)

For production environments with log aggregation:

[source,yaml]
----
logging:
  pattern:
    console: '{"timestamp":"%d{ISO8601}","level":"%level","service":"${spring.application.name}","trace":"%X{traceId}","span":"%X{spanId}","message":"%msg"}%n'
----

== Scheduled Test Execution

Test scenarios with `scheduledEnabled=true` and a `scheduledTime` (HH:mm format) are automatically triggered by `ScheduledScenarioService` every minute.

* The scheduler fires on the top of every minute (`cron = "0 * * * * *"`)
* If a test is already `RUNNING` or `EXPORTING`, the scheduled run is skipped
* Each matched scenario launches in a dedicated virtual thread with a 5-minute timeout
* Results are stored in the `test_run` table; no export ZIP is generated for scheduled runs

To create a scheduled scenario, set `scheduledEnabled: true` and `scheduledTime: "HH:mm"` via the Test Scenarios API.

== Queue Configuration

=== IBM MQ Queue Settings

Queues are configured via MQSC scripts:

.20-create-queues.mqsc
[source,text]
----
DEFINE QLOCAL('DEV.QUEUE.1') MAXDEPTH(50000) REPLACE
DEFINE QLOCAL('DEV.QUEUE.2') MAXDEPTH(50000) REPLACE
----

=== Kafka Topic Settings

Topics are auto-created with default settings or can be pre-configured:

[cols="2,1,1,2"]
|===
|Topic |Partitions |Replication |Retention

|mq-requests
|3
|1
|7 days

|mq-responses
|3
|1
|7 days
|===

== Security Configuration

=== IBM MQ Security

Default development credentials:

[cols="2,3"]
|===
|Setting |Value

|Admin User
|admin

|Admin Password
|passw0rd

|App User
|app

|App Password
|passw0rd
|===

WARNING: Change default passwords for production deployments.

=== API Gateway SSL

The API Gateway is configured to trust self-signed certificates for IBM MQ:

[source,java]
----
@Configuration
public class SslConfig {
    @Bean
    public RestClient.Builder restClientBuilder() {
        return RestClient.builder()
                .requestFactory(createSslTrustingRequestFactory());
    }
}
----

NOTE: This configuration trusts all certificates and should only be used in development/testing environments.
